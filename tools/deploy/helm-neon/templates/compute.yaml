apiVersion: v1
kind: Service
metadata:
  name: compute
  labels:
    app: compute
    component: compute
    part-of: neon
spec:
  ports:
    - name: postgres
      targetPort: postgres
      port: 55433
    - name: http
      targetPort: http
      port: 3080
    # - name: metrics
    #   targetPort: metrics
    #   port: 9187
  selector:
    app: compute
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: compute
  labels:
    app: compute
    component: compute
    part-of: neon
spec:
  replicas: 1
  selector:
    matchLabels:
      app: compute
  template:
    metadata:
      labels:
        app: compute
        component: compute
        part-of: neon
    spec:
      containers:
        - name: compute
          image: ctring/compute-node-v14:latest
          env:
            - name: REGION
              value: {{ .Values.namespace_id | quote }}
            - name: SAFEKEEPERS_ADDR
              value: {{ include "safekeeperNodes" . }}
          command:
            - /bin/sh
            - -ecx
          args:
            - |
              {{- .Files.Get "compute.sh" | indent 14 }}
          volumeMounts:
            - name: config
              mountPath: /config
            - name: data
              mountPath: /data
          ports:
            - name: postgres
              containerPort: 55433
            - name: http
              containerPort: 3080
        # - name: metrics-exporter
        #   image: quay.io/prometheuscommunity/postgres-exporter:master
        #   env:
        #     - name: DATA_SOURCE_NAME
        #       value: postgresql://cloud_admin@localhost:55433/postgres?sslmode=disable
        #   ports:
        #     - name: metrics
        #       containerPort: 9187
      restartPolicy: Always
      volumes:
        - name: config
          configMap:
            name: compute-config
        - name: data
          emptyDir:
            sizeLimit: 40Gi
      affinity:
        nodeAffinity:
          # This rule is useful when we deploy all "regions" in the same physical region, making
          # them not have a division based on cluster context. Therefore, we need to rely on the
          # node labels to separate them by regions.
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - {{- include "nodesInCurrentRegion" . | indent 20 }}
          {{- if eq .Release.Namespace "global" }}
          # In the global region, everything is placed on the "hub" node
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: role
                    operator: In
                    values:
                      - hub
          {{- else }}
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: role
                    operator: In
                    values:
                      - compute
            - weight: 50
              preference:
                matchExpressions:
                  - key: role
                    operator: In
                    values:
                      - pageserver
          {{- end }}

