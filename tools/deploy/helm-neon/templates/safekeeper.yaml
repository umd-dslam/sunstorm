apiVersion: v1
kind: Service
metadata:
  name: safekeeper
  labels:
    app: safekeeper
    component: storage
    part-of: neon
spec:
  ports:
    - name: postgres
      targetPort: postgres
      port: 5454
    - name: http
      targetPort: http
      port: 7676
  selector:
    app: safekeeper
  # In general, service IPs and pod IPs come from different CIDR ranges.
  # In AWS, the pods are assigned IPs direclty from the VPC subnet whereas
  # the service IPs are assigned from a virtual range, which is not easily
  # accessible from a different region. The VPC subnets, however, can be easily
  # routed between regions. Therefore, we use a headless service (setting 
  # clusterIP to None) to expose the pod IPs directly instead of using the service IP.
  clusterIP: None
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: safekeeper
  labels:
    app: safekeeper
    component: storage
    part-of: neon
spec:
  serviceName: "safekeeper"
  replicas: {{ .Values.safekeeper_replicas }}
  selector:
    matchLabels:
      app: safekeeper
  template:
    metadata:
      labels:
        app: safekeeper
        component: storage
        part-of: neon
    spec:
      containers:
        - name: safekeeper
          image: ctring/neon:latest
          env:
            - name: KUBE_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          command:
            - /bin/sh
            - -ecx
          args:           
            - /usr/local/bin/safekeeper 
              --id=${HOSTNAME##*-}
              --datadir=/data
              --advertise-pg=${HOSTNAME}.safekeeper.${KUBE_NAMESPACE}:5454
              --listen-pg=0.0.0.0:5454
              --listen-http=0.0.0.0:7676
              --broker-endpoint=http://storage-broker.global:50051
          volumeMounts:
            - name: data
              mountPath: /data
          ports:
            - name: postgres
              containerPort: 5454
            - name: http
              containerPort: 7676
      volumes:
        - name: data
          emptyDir:
            sizeLimit: 10Gi
      restartPolicy: Always
      affinity:
        nodeAffinity:
          # This rule is useful when we deploy all "regions" in the same physical region, making
          # them not have a division based on cluster context. Therefore, we need to rely on the
          # node labels to separate them by regions.
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - {{- include "nodesInCurrentRegion" . | indent 20 }}
          {{- if eq .Release.Namespace "global" }}
          # In the global region, everything is placed on the "hub" node
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: role
                    operator: In
                    values:
                      - hub
          {{- else }}
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: role
                    operator: In
                    values:
                      - safekeeper
            - weight: 90
              preference:
                matchExpressions:
                  - key: role
                    operator: In
                    values:
                      - compute
            - weight: 50
              preference:
                matchExpressions:
                  - key: role
                    operator: In
                    values:
                      - pageserver
          {{- end }}